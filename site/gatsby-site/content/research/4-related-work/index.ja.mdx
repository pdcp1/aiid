---
title: "関連する研究"
metaTitle: "関連する研究"
metaDescription: "関連する研究"
slug: "/research/4-related-work"
aiTranslated: true
---

公式なAIインシデント研究は比較的新しいものですが、何人かの人々がインシデントと見なされる可能性のある事例を収集しています。これには、

* [素晴らしい機械学習の解釈性: AIインシデントトラッカー](https://github.com/jphall663/awesome-machine-learning-interpretability/blob/master/README.md#ai-incident-tracker)
* [Charlie PownallのAIおよびアルゴリズムインシデントと論争](https://charliepownall.com/ai-algorithimic-incident-controversy-database/)
* [有益なおよび有害なAIのマップ](https://map.ai-global.org/)

ここに追加できるインシデントリソースがあれば、[お問い合わせ](/contact)してください。

以下の出版物は[Google Scholarによってデータベース自体を参照している](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=3482645389524246185)とされていますが、個々のインシデントだけでなく、お手数ですが、お問い合わせいただければ幸いです。[お問い合わせ](/contact)

## 責任あるAI協力的研究

広範な安全性と公平性の共同体に奉仕する必要がある場合、Collabは研究を制作およびスポンサーします。これまでの作業には以下が含まれます。

* AIインシデントデータベースの公開発表時にリリースされたオリジナルな研究出版物。この作業へのすべての引用はこのページに追加されます。  
[McGregor, Sean. "Preventing repeated real-world AI failures by cataloging incidents: The AI incident database." AAAI Conference on Artificial Intelligenceの論文集. Vol. 35. No. 17. 2021.](https://ojs.aaai.org/index.php/AAAI/article/download/17817/17624)
* [2022 NeurIPS Workshop on Human-Centered AI](https://nips.cc/virtual/2022/workshop/50008)で提示されたインシデントの定義と基準の主要な更新。  
[McGregor, Sean, Kevin Paeth, and Khoa Lam. "Indexing AI Risks with Incidents, Issues, and Variants." arXiv preprint arXiv:2211.10384 (2022).](https://arxiv.org/pdf/2211.10384)
* オープンソースのインシデントレポートを分析する際のインシデントの原因の不確実性を減少させるアプローチ。[SafeAI](https://safeai.webs.upv.es/)で発表。  
[Pittaras, Nikiforos, and Sean McGregor. "A taxonomic system for failure cause analysis of open source AI incidents." arXiv preprint arXiv:2211.07280 (2022).](https://arxiv.org/pdf/2211.07280)

## 2023年（2月24日まで）

* [McGregor, Sean, and Jesse Hostetler. "データ中心のガバナンス." arXiv プリプリント arXiv:2302.07872 (2023).](https://arxiv.org/pdf/2302.07872)
* [NIST. リスクマネジメントプレイブック. 2023](https://pages.nist.gov/AIRMF/)

## 2022年

* [Macrae, Carl. "自律型および知能型システムの失敗からの学び：事故、安全性、および社会技術的なリスクの分析." リスク分析 42.9 (2022): 1999-2025.](https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/risa.13850)
* [Felländer, Anna, et al. "倫理的AIのためのデータ駆動型リスク評価手法の達成." Digital Society 1.2 (2022): 13.](https://link.springer.com/article/10.1007/s44206-022-00016-0)
* [Apruzzese, Giovanni, et al. ""実際の攻撃者は勾配を計算しない"：敵対的なML研究と実践のギャップを埋める." arXiv プリプリント arXiv:2212.14315 (2022).](https://arxiv.org/pdf/2212.14315)
* [Petersen, Eike, et al. "医学のための責任あるおよび規制適合機械学習：課題と解決策の調査." IEEE Access 10 (2022): 58375-58418.](https://ieeexplore.ieee.org/iel7/6287639/9668973/09783196.pdf)
* [Schuett, Jonas. "AIからのリスクに対する三つの防御ライン." arXiv プリプリント arXiv:2212.08364 (2022).](https://arxiv.org/pdf/2212.08364)
* [Schiff, Daniel S. "色付きのメガネで政策ウィンドウを見る：米国のAI政策の議題設定." レビュー・オブ・ポリシー・リサーチ.](https://onlinelibrary.wiley.com/doi/abs/10.1111/ropr.12535)
* [Neretin, Oleksii, and Vyacheslav Kharchenko. "Big Dataツールを使用したAIシステムの脆弱性収集と分析のプロセスを記述するモデル." 2022年第12回信頼性システム、サービス、および技術国際会議（DESSERT）. IEEE、2022.](https://ieeexplore.ieee.org/abstract/document/10018811/)
* [Durso, Francis, et al. "人工知能学習システムにおける障害の分析（FAILS）." 2022 IEEE 29th Annual Software Technology Conference (STC). IEEE、2022.](https://ieeexplore.ieee.org/abstract/document/9951011/)
* [Kassab, Mohamad, Joanna DeFranco, and Phillip Laplante. "AIインフューズドシステムのバグの調査：分析と提案された分類." 2022 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW). IEEE、2022.](https://ieeexplore.ieee.org/abstract/document/9985178/)
* [Braga, Juliao, et al. "アルゴリズムとデータのガバナンスに関する論文の開発プロジェクト." (2022).](https://osf.io/xcpsd/download)
* [Secchi, Carlo, and Alessandro Gili. "持続可能なインフラのためのデジタル化：前途." 持続可能なインフラのためのデジタル化（2022）：1-326.](https://www.torrossa.com/it/resources/an/5394879)
* [Groza, Adrian, et al. "Elaborarea cadrului strategic nat, ional în domeniul inteligent, ei artificiale."](https://www.adr.gov.ro/wp-content/uploads/2022/03/Analiza-reglementarilor-pentru-domeniul-inteligentei-artificiale.pdf)
* [Braga, Juliao, et al. "アルゴリズムとデータガバナンスに関する論文の開発プロジェクト." (2022).](https://osf.io/sr7kt/download)（[元のポルトガル語](https://osf.io/xcpsd/download)）.
* [NIST. リスクマネジメントプレイブック. 2022](https://pages.nist.gov/AIRMF/)
* [Shneiderman, Ben. Human-Centered AI. Oxford University Press, 2022.](https://www.amazon.com/Human-Centered-AI-Ben-Shneiderman/dp/0192845292)  
* [Schwartz, Reva, et al. "人工知能における偏見の特定および管理のための標準に向けて." (2022).](https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=934464)  
* [McGrath, Quintin et al. "プロ倫理的AIソリューションを設計するためのエンタープライズリスク管理フレームワーク." University of South Florida. (2022)](https://www.usf.edu/business/documents/desrist/paper_65.pdf).  
* [Nor, Ahmad Kamal Mohd, et al. "説明可能なベイジアンディープラーニングを使用した異常検知と故障予測：実世界のガスタービンの異常の方法論と事例研究." (2022).](https://www.mdpi.com/2227-7390/10/4/554/pdf)  
* [Xie, Xuan, Kristian Kersting, and Daniel Neider. "ディープニューラルネットワークの神経記号的検証." arXiv プリプリント arXiv:2203.00938 (2022).](https://arxiv.org/pdf/2203.00938)  
* [Hundt, Andrew, et al. "ロボットが悪意あるステレオタイプを実行する." 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022.](https://dl.acm.org/doi/fullHtml/10.1145/3531146.3533138)  
* [Tidjon, Lionel Nganyewou, and Foutse Khomh. "機械学習ベースのシステムにおける脅威評価." arXiv プリプリント arXiv:2207.00091 (2022).](https://arxiv.org/pdf/2207.00091)  
* [Naja, Iman, et al. "ナレッジグラフを使用したAIの説明責任情報の実用的な収集、統合、および監査の解除." IEEE Access 10 (2022): 74383-74411.](https://ieeexplore.ieee.org/iel7/6287639/9668973/09815594.pdf)  
* [Cinà, Antonio Emanuele, et al. "ワイルドパターンリローデッド：機械学習セキュリティに対するトレーニングデータの汚染の調査." arXiv プリプリント arXiv:2205.01992 (2022).](https://arxiv.org/pdf/2205.01992)  
* [Schröder, Tim, and Michael Schulz. "機械学習モデルの監視：課題と方法のカテゴリ化." データサイエンスとマネジメント（2022）.](https://www.sciencedirect.com/science/article/pii/S2666764922000303)  
* [Corea, Francesco, et al. "AIに対する原則ベースのアプローチ：欧州連合およびイタリアの事例." AI & SOCIETY (2022): 1-15.](https://link.springer.com/article/10.1007/s00146-022-01453-8)  
* [Carmichael, Zachariah, and Walter J. Scheirer. "アンフーリング摂動ベースの事後説明." arXiv プリプリント arXiv:2205.14772 (2022).](https://arxiv.org/pdf/2205.14772)  
* [Wei, Mengyi, and Zhixuan Zhou. "現実世界のAI倫理問題：AIインシデントデータベースからのエビデンス." arXiv プリプリント arXiv:2206.07635 (2022).](https://arxiv.org/pdf/2206.07635)  
* [Petersen, Eike, et al. "医学のための責任あるおよび規制適合機械学習：課題と解決策の調査." IEEE Access (2022).](https://ieeexplore.ieee.org/iel7/6287639/9668973/09783196.pdf)  
* [Karunagaran, Surya, Ana Lucic, and Christine Custis. "XAI Toolsheet: XAIツールのためのドキュメンテーションフレームワークに向けて."](https://a-lucic.github.io/talks/xai_pai_toolsheets.pdf)  
* [Paudel, Shreyasha, and Aatiz Ghimire. "ネパールにおけるAI倫理調査."](https://www.naamii.org.np/wp-content/uploads/2021/11/AI-Ethics-Survey-Report.pdf)  
* [Ferguson, Ryan. "ニューラルネットワークを使用したリスクプロセスの変革."](https://www.firm.fm/wp-content/uploads/2022/05/Papers-Round-Table-AI-April-2022.pdf)  
* [Fujitsu Corporation. "AI倫理影響評価ケースブック," 2022](https://www.fujitsu.com/global/documents/about/research/technology/aiethics/fujitsu-AIethics-case_en.pdf)  
* [Shneiderman, Ben and Du, Mengnan. "Human-Centered AI: Tools" 2022](https://hcai.site/tools/)  
* [Salih, Salih. "機械学習の解釈可能性を理解する." Medium. 2022](https://towardsdatascience.com/understanding-machine-learning-interpretability-168fd7562a1a)  
* [Garner, Carrie. "変革的で信頼性のあるAIシステムを作成するには、コミュニティの協力が必要です." Software Engineering Institute. 2022](https://insights.sei.cmu.edu/blog/creating-transformative-and-trustworthy-ai-systems-requires-a-community-effort/)  
* [Weissinger, Laurin, AI, Complexity, and Regulation (February 14, 2022). The Oxford Handbook of AI Governance](https://academic.oup.com/edited-volume/41989)

## 2021

* [Arnold, Z., Toner, H., CSET Policy. "AI Accidents: An Emerging Threat." (2021).](https://cset.georgetown.edu/wp-content/uploads/CSET-AI-Accidents-An-Emerging-Threat.pdf)  
* [Aliman, Nadisha-Marie, Leon Kester, and Roman Yampolskiy. "トランスディシプリナリーAIオブザーバトリー—回顧的分析と将来志向の対照." Philosophies 6.1 (2021): 6.](https://www.mdpi.com/2409-9287/6/1/6/pdf)  
* [Falco, Gregory, and Leilani H. Gilpin. "自律システムの検証および検証（V＆V）のためのストレステストフレームワーク." 2021 IEEE International Conference on Autonomous Systems (ICAS). IEEE, 2021.](https://www.researchgate.net/profile/Gregory-Falco/publication/352930787_A_Stress_Testing_Framework_For_Autonomous_System_Verification_And_Validation_VV/links/60e911fcb8c0d5588ce64ec5/A-Stress-Testing-Framework-For-Autonomous-System-Verification-And-Validation-V-V.pdf)  
* [Petersen, Eike, et al. "医学のための責任あるおよび規制適合機械学習：技術的課題と解決策の調査." arXiv プリプリント arXiv:2107.09546 (2021).](https://arxiv.org/pdf/2107.09546)  
* [John-Mathews, Jean-Marie. AI倫理の実践、課題、および制約. Diss. Université Paris-Saclay, 2021.](https://tel.archives-ouvertes.fr/tel-03527232/document)  
* [Macrae, Carl. "自律および知的システムの失敗からの学び：事故、安全性、および社会技術的リスクの源." Safety and Sociotechnical Sources of Risk (June 4, 2021) (2021).](https://nottingham-repository.worktribe.com/OutputFile/7164920)  
* [Hong, Matthew K., et al. "自然言語の失敗に備えたAIプレイブック." Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 2021.](https://www.adamfourney.com/papers/hong_chi2021.pdf)  
* [Ruohonen, Jukka. "ヨーロッパ連合における製品安全規制のレビュー." arXiv プリプリント arXiv:2102.03679 (2021).](https://arxiv.org/pdf/2102.03679)  
* [Kalin, Josh, David Noever, および Matthew Ciolino. "マシンラーニングモデルへの敵対的リスクを評価するための修正されたドレイク方程式." arXiv プリプリント arXiv:2103.02718 (2021).](https://arxiv.org/pdf/2103.02718)  
* [Aliman, Nadisha Marie, および Leon Kester. "科学的および経験的な敵対的AI攻撃に対するエピステミックな防御." CEUR Workshop Proceedings. Vol. 2916. CEUR WS, 2021.](https://dspace.library.uu.nl/bitstream/handle/1874/413353/paper_1.pdf?sequence=1)  
* [John-Mathews, Jean-Marie. L’Éthique de l’Intelligence Artificielle en Pratique. Enjeux et Limites. Diss. université Paris-Saclay, 2021.](https://www.theses.fr/2021UPASI015.pdf)  
* [Smith, Catherine. "知的自由の自動化：人工知能、偏見、および情報の風景." IFLA Journal (2021): 03400352211057145](https://journals.sagepub.com/doi/abs/10.1177/03400352211057145)


もしここに追加すべき学術的な研究があれば、[お問い合わせ](/contact) からご連絡ください。
